\documentclass[a4paper, 12pt,oneside]{article} 
%\documentclass[a4paper, 12pt,oneside,draft]{article} 
\usepackage{preamble}
%--------------------- ACTUAL FILE ---------------------- %
\begin{document} 
%%%
	\input{title_page} 
	% Add titlepage
	\clearpage
	\tableofcontents
	\thispagestyle{empty}
	% Add table of contents
	\clearpage
	\pagenumbering{arabic}
	\setcounter{page}{1}
	\section*{Abstract}
		context of the original paper (with their claims). The paper that won the 2024 IgNobel Prize in Probability took a Bayesian approach to studying the statistics behind the coin-flipping process. Its main goal was to confirm a prediction made by a physical model of human coin tossing developed by Diaconis, Holmes, and Montgomery (DHM; 2007); i.e. that when people flip an ordinary coin, the probability of it landing on the same side it started is about 51\%. 
		It also revealed considerable between-people variation in the degree of this same-side bias, as well as its decrease as more coins were flipped. 

		The goal of this report is two fold. On the one side, we aim to investigate similar questions with a regression approach : 
		is there evidence for between-person, between-coin, or even person-coin pair differences ? To what extent does flipping experience affect the observed same-side bias ?
		In addition, we also investigate the differences between GLM and WLS approaches; as well as muscle memory effects (through outcomes of recent flips). 
	\section{Introduction}
		Before diving into the analysis, we provide a brief overview of the datasets main features and the models we consider. Our exploratory analysis is available as a Jupyter notebook, and provides more detail. 

		The dataset is composed of throws from 48 people using 44 coins in total. Given the study did not impose strict guidelines for coins to be used, the design is heavily unbalanced. Eighteen coins have only been thrown by a single person, while some of them have been thrown by more than 20 people. Also, more than half the people have only flipped 5 or less different coins, while someone threw 11 different ones. As for the the person-coin pairs, most have fewer or equal to 1000 throws, while some have around 10000 ones. This severe unbalance must be kept in mind during the study, given it can pose challenges during model interpretation. 

		After plotting the same-side rates across people, coin and person-coin combinations, we deemed it relevant to investigate models considering both person and coin as covariates, as well as individual person-coin pairs. Following the advice given on the project statement, we also branched our analysis in Binomial-response GLM and WLS approaches. 

		Motivated by impacts of muscle-memory on the flipping, we additionally investigated aspects such as time-varying same-side rates and memory between successive throws.
	\section{Analysis}
		\subsection{Model Comparison}
			In this section, we introduce and compare different models for the same side success rate. 

			Some GLMs with binomial responses and some WLS ones based on the ... approximation.
			
			Should explain no a priori response transformation ...
			
			For each, the considered formulas in terms of the covariates are:
			\begin{itemize}
				\item \texttt{1}, corresponding to a constant model.
				\item \texttt{1+C(person)}, corresponding to a model with the person as a covariate.
				\item \texttt{1+C(person)+C(coin)}, corresponding to a model with the person and the coin as covariates.
				\item \texttt{1+C(person)+C(coin)+C(person):C(coin)}, corresponding to a model with the person, the coin, and the interaction between the person and the coin as covariates.
			\end{itemize}
			* model 4 could seem redundant due to nesting-main effect, but we ....
			*  

			Should explain why eliminated some covariates ...
			\subsubsection{Tools For Selection}
			\begin{itemize}
				\item when to use AIC vs LRT ?
				\item citing a few things about LRT not miting overfitting 
				\item 
			\end{itemize}
			\subsubsection{Diagnostic Plots}
			\begin{itemize}
				\item describe the plots we choose 
				\item explain what normal behaviour looks like on them
				\item explain what problematic behaviour looks like on them
			\end{itemize}
			\subsubsection{WLS Approach}
			In this section, we consider the normal approximation for the binomial variable $R$ with denominator $m$. Having that the success probability is fairly close to 0.5 we get $p(1-p)$ approximately equal to 1/4. Hence we end up with: $R/m \sim N(p,1/(4m))$. 

			We then make a linear fit using weighted least squares, where  weight associated to each entry is proportional to the inverse of the variance, that only depending on $m$.

			We consider different models, starting with a constant model, then adding in the following order the 

			\begin{table}[htb]
				\centering
				\caption{Model comparison based on AIC values.}
				\label{tab:model-comparison}
				\begin{tabular}{lc}
				\toprule
				Model Formula (RHS) & AIC \\
				\midrule
				1 & 159.76 \\
				1 + C(person) & 12.89 \\
				1 + C(person) + agg & 0.00 \\
				1 + C(person) + agg + C(coin) & 29.21 \\
				1 + C(person) + agg + C(person):C(coin) & 123.98 \\
				\bottomrule
				\end{tabular}
			\end{table}
			\subsubsection{GLM Approach}
			In this section, instead of relying on an approximation to fit a Least Squares models, we take into account the nature of the data by using a GLM. Indeed, given the binomial nature of aggregated data, using a binomial-response GLM seems natural. 
			\begin{table}[htb]
				\centering
				\caption{Model comparison for different models.}
				\label{tab:model-comparison}
				\begin{tabular}{lccc}
				\toprule
				Model & Deviance & AIC & Model DF \\
				\midrule
				\texttt{1} & 3942.13 & 187.84 & 0 \\
				\texttt{1+person} & 3676.20 & 13.91 & 46 \\
				\texttt{1+person+agg} & 3660.29 & 0.00 & 47 \\
				\texttt{1+person+agg+coin} & 3602.26 & 25.97 & 89 \\
				\bottomrule
				\end{tabular}
			\end{table}
			More specifically, we consider a Logit link as it leads to easily interpretable results, meaning we model $\mathbb{E}[y\mid x]=\exp\left(\sum_i \beta_i x_i\right)$ with binomial errors. We expect this to lead to more accurate results than the WLS approximation, especially for entries having $R/M\not\approx 1/2$.
			\begin{table}[htb]
				\centering
				\caption{Likelihood ratio tests between models.}
				\label{tab:llr-comparison}
				\begin{tabular}{llc}
				\toprule
				Tested model & Restricted model & $p$-value \\
				\midrule
				\texttt{1+person} & \texttt{1} & 0.00e+00 \\
				\texttt{1+person+agg} & \texttt{1+person} & 6.63e-05 \\
				\texttt{1+person+agg+coin} & \texttt{1+person+agg} & 5.09e-02 \\
				\bottomrule
				\end{tabular}
			\end{table}
			As for the considered covariates, we follow what is done for the WLS approach. The only difference being in the interpretation of the coefficients and the in the fact that we omit the model with coins nested within people. Indeed, even the \texttt{0+person:coin} model did not give any signs of convergence after 100 IRLS iterations. We tried fitting it with BFGS, but did not succeed either as the hessian resulted to be non-full rank. The analysis of deviance and LRT tables for the models that converged are \ref{tab:model-comparison} and \ref{tab:llr-comparison}.
			\begin{figure}[htb]
				\centering
				\includegraphics[width=0.85\textwidth]{GLM_diagnostics.png}
				\caption{Diagnostics for the selected GLM model. (a).}
				\label{fig:glm-diagnostic}
			\end{figure}
			The diagnostic plots for the \texttt{1+person+agg+coin} model are in \ref{fig:glm-diagnostic} and \ref{fig:dev-resid-vs-covariates}. 
			\begin{figure}[htb]
				\centering
				\includegraphics[width=0.85\textwidth]{dev_resid_vs_covariates.png}
				\caption{Dev-resid as a function of (a) person and (b) coin.}
				\label{fig:dev-resid-vs-covariates}
			\end{figure}	
		\subsection{Unusual Observations}

		\subsection{Zoom on Learning Effects}
		\begin{itemize}
			\item bias comes from start
			\item amount of bias (considerable)
			\item wobble interpretation (consistent with physical model, citing the paper)
		\end{itemize}
		\lipsum[1]
		\begin{figure}[htb]
			\centering
			\includegraphics[width=0.5\textwidth]{learning_effects.png}
			\caption{Learning effects.}
			\label{fig:learning-effects}
		\end{figure}
		\subsection{Memory Effects}
		In this section we shift our focus to memory effects. We do so motivated by the fact that we deem it probable a priori that successive throws are more similar to each other than randomly selected ones. Indeed, one could imagine that after two same-side throws, the next could end being a same-side one two with a probability higher than the base rate. 
		
		To test this we start by considering the data consisting of individual throw outcomes. To this, we add columns corresponding to 
		\begin{itemize}
			\item same-side indicator variables,
			\item same-side indicator variables for the penultimate throw,
			\item same-side indicator variables for the antepenultimate throw.
		\end{itemize}
		To deal with the boundary effects between sequences of flips, we removed the two first entries of each sequence. 
		%To speed up the computations
		
		We then define the models 
		\begin{itemize}
			\item \texttt{1}, same-side indicator variables,
			\item \texttt{1+hop1\_mem}, same-side indicator variables for the penultimate throw,
			\item \texttt{1+hop1\_mem+hop2\_mem}, same-side indicator variables for the antepenultimate throw.
		\end{itemize}
		and carry out an analysis of deviance. The results associated are found in \ref{tab:memory-model-comparison}. Given the uni-directionality of these results, no further analysis was made.  
		\begin{table}[htb]
			\centering
			\caption{Model comparison for models including : no memory, 1-hop memory and 2-hop memory.}
			\label{tab:memory-model-comparison}
			\begin{tabular}{lccc}
			\toprule
			Model & Deviance & AIC & Model DF \\
			\midrule
			\texttt{1} & 474381.54 & 0.00 & 0 \\
			\texttt{1+hop1\_mem} & 474380.73 & 1.19 & 1 \\
			\texttt{1+hop1\_mem+hop2\_mem} & 474380.53 & 2.98 & 2 \\
			\bottomrule
			\end{tabular}
		\end{table}
	\section{Discussion}
		\subsection{Model Comparison}
		\subsubsection{WLS Approach}
		\subsubsection{GLM Approach}

		\subsection{Unusual Observations}

		\subsection{Zoom on Learning Effects}
		\subsection{Memory Effects}
		Looking at \ref{tab:memory-model-comparison}, we see there is no support in favour of memory effects relative to the constant model. The deviance decreases by around .8 due to the introduction of the penultimate throw memory and only a further .2 when including memory of the antepenultimate throw. Another factor that shows this is the increase by $>$1 and $\approx 3$ respectively in AIC compared to the constant model.
		The fact that memory about antepenultimate outcome seems to matter even less than memory about the penultimate outcome does make nonetheless intuitive sense. 

		This (non-) finding can in itself be regarded as reassuring in a way. Specifically, it might contribute to rule out concerns of the authors of the original paper regarding the potential same-side bias induced by participants knowing about the goal of the study. Indeed it seems far-fetched that someone could bias their throws without relying on muscle memory. 
	\section{Conclusion}
	\section*{Acknowledgements}
	\section*{References}
	%\appendix
	%	\section{Runtime Estimation}\label{appendix:runtime_estimation}
%%%
\end{document} 